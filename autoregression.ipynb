{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "model_checkpoint = \"TRL_2.9398_TSL_3.2146_EMB_768_LYR_5_HDS_16_CTX_128_LR_0.0001.pth\"\n",
        "size = os.stat(model_checkpoint).st_size\n",
        "\n",
        "while True:\n",
        "  time.sleep(60)\n",
        "  new_size = os.stat(model_checkpoint).st_size\n",
        "  if new_size == size:\n",
        "    break\n",
        "  size = new_size"
      ],
      "metadata": {
        "id": "mcgSrvMevJXo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5AGUOI1L5KD8"
      },
      "outputs": [],
      "source": [
        "import platform\n",
        "\n",
        "if not os.path.exists(\"./datasets.zip\"):\n",
        "    print(\"upload datasets\")\n",
        "elif platform.system() == \"Windows\":\n",
        "    os.system(\"git clone https://github.com/n1teshy/poet & move poet/tokenizer . & move poet/core . & rd /s /q poet\")\n",
        "    os.system(\"powershell Expand-Archive -Path ./datasets -DestinationPath .\")\n",
        "else:\n",
        "    os.system(\"git clone https://github.com/n1teshy/poet && mv poet/tokenizer . && mv poet/core . && rm -rf poet\")\n",
        "    os.system(\"unzip ./datasets -d .\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8odFqXR_5KD-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from google.colab import drive\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from core.tokenizers.regex import get_tokenizer\n",
        "from core.utils import get_param_count\n",
        "from core.config import device\n",
        "from core.models import Generator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "Sqy49gjfabQU",
        "outputId": "f2471167-742a-470d-de8d-1d192fa54f99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir drive/MyDrive/poet_params -p"
      ],
      "metadata": {
        "id": "9ow0ataBbPHt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AUO5S6T25KD_"
      },
      "outputs": [],
      "source": [
        "BLOCK_SIZE = 128\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "EMBEDDING_SIZE = 768\n",
        "LAYERS = 5\n",
        "HEADS = 16\n",
        "TRAIN_FILE = \"datasets/train.txt\"\n",
        "TEST_FILE = \"datasets/test.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CUOXqa8W5KD_"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_tokenizer(\"poems.txt\", 1024, \"tokenizer/en\", True)\n",
        "train_data = tokenizer.encode(open(TRAIN_FILE, encoding=\"utf-8\").read())\n",
        "test_data = tokenizer.encode(open(TEST_FILE, encoding=\"utf-8\").read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "U7GLaFCA5KD_"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else test_data\n",
        "    idxs = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE, ))\n",
        "    X = [data[idx: idx + BLOCK_SIZE] for idx in idxs]\n",
        "    Y = [data[idx + 1: idx + 1 + BLOCK_SIZE] for idx in idxs]\n",
        "    return torch.tensor(X, device=device), torch.tensor(Y, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zRt7eZdw5KD_",
        "outputId": "f4428dcc-26eb-4096-e731-5aa84fecb6f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37.0255 mn parameters\n"
          ]
        }
      ],
      "source": [
        "model = Generator(tokenizer.size, EMBEDDING_SIZE, BLOCK_SIZE, LAYERS, HEADS).to(device)\n",
        "model.load_state_dict(torch.load(model_checkpoint))\n",
        "print(\"%.4f mn parameters\" % (get_param_count(model) / 1e6, ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "G0ybUS9_5KEA"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def get_test_loss():\n",
        "    model.eval()\n",
        "    inp, tgt = get_batch(\"test\")\n",
        "    logits = model(inp)\n",
        "    B, T, C = logits.shape\n",
        "    logits, tgt = logits.reshape(B*T, C), tgt.reshape(B*T)\n",
        "    loss = F.cross_entropy(logits, tgt)\n",
        "    model.train()\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VAMmZ4gv5KEA"
      },
      "outputs": [],
      "source": [
        "def batch_generator(split):\n",
        "    while True:\n",
        "        yield get_batch(split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pPQxuMFA5KEA"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.0001\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1-1VY8BC5KEA"
      },
      "outputs": [],
      "source": [
        "mean_train_loss, mean_test_loss = 2.95, 0\n",
        "batch_to_epoch = len(train_data) / BATCH_SIZE\n",
        "last_saved_train_loss = 2.9398"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(folder):\n",
        "    model_id = \"TRL_%.4f_TSL_%.4f_EMB_%d_LYR_%d_HDS_%d_CTX_%d_LR_%.4f\" % (\n",
        "        mean_train_loss,\n",
        "        mean_test_loss,\n",
        "        EMBEDDING_SIZE,\n",
        "        LAYERS,\n",
        "        HEADS,\n",
        "        BLOCK_SIZE,\n",
        "        LEARNING_RATE,\n",
        "    )\n",
        "    torch.save(model.state_dict(), os.path.join(folder, f\"{model_id}.pth\"))\n"
      ],
      "metadata": {
        "id": "YQt-BiykOlFM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN7Y1tte5KEA",
        "outputId": "07366e0e-540b-405b-edde-f6b9b9cf2c43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1:1 -> (2.9211 | 2.9499), (3.2631 | 3.2631)\n",
            "1:2 -> (2.9634 | 2.9500), (3.1450 | 3.2628)\n",
            "1:3 -> (2.9435 | 2.9499), (3.2444 | 3.2627)\n",
            "1:4 -> (2.9683 | 2.9500), (3.2153 | 3.2626)\n",
            "1:5 -> (2.9657 | 2.9500), (3.1942 | 3.2624)\n",
            "1:6 -> (2.9575 | 2.9500), (3.2780 | 3.2625)\n",
            "1:7 -> (2.9300 | 2.9500), (3.2387 | 3.2624)\n",
            "1:8 -> (2.9301 | 2.9499), (3.1663 | 3.2622)\n",
            "1:9 -> (2.9663 | 2.9500), (3.2526 | 3.2621)\n",
            "1:10 -> (2.9655 | 2.9500), (3.2225 | 3.2620)\n",
            "1:11 -> (2.9503 | 2.9500), (3.1842 | 3.2618)\n",
            "1:12 -> (2.9246 | 2.9500), (3.1845 | 3.2617)\n",
            "1:13 -> (2.8518 | 2.9497), (3.1133 | 3.2613)\n",
            "1:14 -> (2.9970 | 2.9498), (3.1614 | 3.2610)\n",
            "1:15 -> (2.9131 | 2.9497), (3.2741 | 3.2611)\n",
            "1:16 -> (2.9601 | 2.9498), (3.2871 | 3.2611)\n",
            "1:17 -> (2.9142 | 2.9497), (3.2707 | 3.2612)\n",
            "1:18 -> (2.8950 | 2.9495), (3.1659 | 3.2609)\n",
            "1:19 -> (2.9310 | 2.9495), (3.2435 | 3.2609)\n",
            "1:20 -> (2.8515 | 2.9493), (3.2222 | 3.2608)\n",
            "1:21 -> (2.9836 | 2.9493), (3.2677 | 3.2608)\n",
            "1:22 -> (2.9798 | 2.9494), (3.3015 | 3.2609)\n",
            "1:23 -> (2.9418 | 2.9494), (3.2223 | 3.2608)\n",
            "1:24 -> (2.9576 | 2.9494), (3.1644 | 3.2606)\n",
            "1:25 -> (2.9725 | 2.9495), (3.1831 | 3.2604)\n",
            "1:26 -> (2.9194 | 2.9494), (3.2244 | 3.2603)\n",
            "1:27 -> (2.8846 | 2.9492), (3.2940 | 3.2604)\n",
            "1:28 -> (3.0157 | 2.9494), (3.2482 | 3.2603)\n",
            "1:29 -> (2.9753 | 2.9495), (3.2591 | 3.2603)\n",
            "1:30 -> (2.9281 | 2.9494), (3.2624 | 3.2603)\n",
            "1:31 -> (2.9337 | 2.9494), (3.2052 | 3.2602)\n",
            "1:32 -> (2.9635 | 2.9494), (3.2708 | 3.2602)\n",
            "1:33 -> (2.8886 | 2.9493), (3.2045 | 3.2601)\n",
            "1:34 -> (2.9135 | 2.9492), (3.2244 | 3.2600)\n",
            "1:35 -> (2.9018 | 2.9491), (3.2260 | 3.2599)\n",
            "1:36 -> (2.9431 | 2.9490), (3.1866 | 3.2597)\n",
            "1:37 -> (2.9864 | 2.9491), (3.1805 | 3.2595)\n",
            "1:38 -> (2.9372 | 2.9491), (3.2596 | 3.2595)\n",
            "1:39 -> (2.8818 | 2.9489), (3.2539 | 3.2595)\n",
            "1:40 -> (2.9444 | 2.9489), (3.2528 | 3.2595)\n",
            "1:41 -> (2.9270 | 2.9489), (3.1810 | 3.2593)\n",
            "1:42 -> (2.9034 | 2.9488), (3.2080 | 3.2592)\n",
            "1:43 -> (2.9001 | 2.9486), (3.1613 | 3.2589)\n",
            "1:44 -> (2.9109 | 2.9485), (3.2117 | 3.2588)\n",
            "1:45 -> (2.9063 | 2.9484), (3.2193 | 3.2587)\n",
            "1:46 -> (2.9096 | 2.9483), (3.1740 | 3.2585)\n",
            "1:47 -> (2.9589 | 2.9484), (3.1833 | 3.2583)\n",
            "1:48 -> (2.9654 | 2.9484), (3.2078 | 3.2582)\n",
            "1:49 -> (2.9616 | 2.9484), (3.2277 | 3.2581)\n",
            "1:50 -> (3.0329 | 2.9486), (3.2298 | 3.2580)\n",
            "1:51 -> (2.9590 | 2.9487), (3.1813 | 3.2578)\n",
            "1:52 -> (2.9065 | 2.9486), (3.1765 | 3.2576)\n",
            "1:53 -> (2.9082 | 2.9485), (3.1741 | 3.2574)\n",
            "1:54 -> (2.8905 | 2.9483), (3.1799 | 3.2572)\n",
            "1:55 -> (2.9359 | 2.9483), (3.1785 | 3.2570)\n",
            "1:56 -> (2.9129 | 2.9482), (3.2169 | 3.2569)\n",
            "1:57 -> (2.9154 | 2.9481), (3.2101 | 3.2568)\n",
            "1:58 -> (2.9342 | 2.9481), (3.2271 | 3.2567)\n",
            "1:59 -> (2.9608 | 2.9481), (3.2158 | 3.2566)\n",
            "1:60 -> (2.9604 | 2.9481), (3.1270 | 3.2563)\n",
            "1:61 -> (2.8583 | 2.9479), (3.2667 | 3.2563)\n",
            "1:62 -> (2.9369 | 2.9479), (3.1387 | 3.2561)\n",
            "1:63 -> (2.9159 | 2.9478), (3.1853 | 3.2559)\n",
            "1:64 -> (2.8736 | 2.9476), (3.2017 | 3.2557)\n",
            "1:65 -> (2.9557 | 2.9477), (3.1489 | 3.2555)\n",
            "1:66 -> (2.9622 | 2.9477), (3.2418 | 3.2554)\n",
            "1:67 -> (2.9339 | 2.9477), (3.2157 | 3.2553)\n",
            "1:68 -> (2.9791 | 2.9477), (3.2860 | 3.2554)\n",
            "1:69 -> (2.9320 | 2.9477), (3.2151 | 3.2553)\n",
            "1:70 -> (2.9227 | 2.9476), (3.3126 | 3.2555)\n",
            "1:71 -> (2.9629 | 2.9477), (3.2331 | 3.2554)\n",
            "1:72 -> (2.9061 | 2.9476), (3.1928 | 3.2552)\n",
            "1:73 -> (2.9084 | 2.9475), (3.2342 | 3.2552)\n",
            "1:74 -> (2.8959 | 2.9473), (3.2045 | 3.2551)\n",
            "1:75 -> (2.9254 | 2.9473), (3.2492 | 3.2551)\n",
            "1:76 -> (2.9042 | 2.9472), (3.1517 | 3.2548)\n",
            "1:77 -> (2.9678 | 2.9472), (3.2002 | 3.2547)\n",
            "1:78 -> (2.9410 | 2.9472), (3.2303 | 3.2546)\n",
            "1:79 -> (2.9553 | 2.9472), (3.2721 | 3.2546)\n",
            "1:80 -> (2.9699 | 2.9473), (3.1555 | 3.2544)\n",
            "1:81 -> (2.8884 | 2.9471), (3.1329 | 3.2541)\n",
            "1:82 -> (2.8995 | 2.9470), (3.2364 | 3.2540)\n",
            "1:83 -> (2.8868 | 2.9469), (3.2099 | 3.2539)\n",
            "1:84 -> (2.8943 | 2.9467), (3.1950 | 3.2538)\n",
            "1:85 -> (3.0100 | 2.9469), (3.1331 | 3.2535)\n",
            "1:86 -> (2.9936 | 2.9470), (3.2513 | 3.2535)\n",
            "1:87 -> (2.9698 | 2.9471), (3.2463 | 3.2535)\n",
            "1:88 -> (2.9020 | 2.9470), (3.1986 | 3.2533)\n",
            "1:89 -> (2.9825 | 2.9470), (3.2375 | 3.2533)\n",
            "1:90 -> (2.8987 | 2.9469), (3.2283 | 3.2532)\n",
            "1:91 -> (2.9993 | 2.9471), (3.3007 | 3.2533)\n",
            "1:92 -> (2.9247 | 2.9470), (3.2233 | 3.2533)\n",
            "1:93 -> (2.9683 | 2.9471), (3.3095 | 3.2534)\n",
            "1:94 -> (2.8565 | 2.9468), (3.2364 | 3.2534)\n",
            "1:95 -> (2.9468 | 2.9468), (3.1818 | 3.2532)\n",
            "1:96 -> (2.9238 | 2.9468), (3.2434 | 3.2532)\n",
            "1:97 -> (2.9269 | 2.9467), (3.2613 | 3.2532)\n",
            "1:98 -> (2.9705 | 2.9468), (3.2288 | 3.2531)\n",
            "1:99 -> (2.9551 | 2.9468), (3.1286 | 3.2528)\n",
            "1:100 -> (2.9740 | 2.9469), (3.2498 | 3.2528)\n",
            "1:101 -> (2.9604 | 2.9469), (3.3090 | 3.2529)\n",
            "1:102 -> (2.8995 | 2.9468), (3.1984 | 3.2528)\n",
            "1:103 -> (2.9473 | 2.9468), (3.2006 | 3.2527)\n",
            "1:104 -> (2.9783 | 2.9469), (3.2087 | 3.2526)\n",
            "1:105 -> (2.9826 | 2.9470), (3.2824 | 3.2526)\n",
            "1:106 -> (2.9586 | 2.9470), (3.2224 | 3.2526)\n",
            "1:107 -> (2.9173 | 2.9469), (3.2657 | 3.2526)\n",
            "1:108 -> (2.9762 | 2.9470), (3.1696 | 3.2524)\n",
            "1:109 -> (2.9109 | 2.9469), (3.2138 | 3.2523)\n",
            "1:110 -> (2.9296 | 2.9468), (3.1753 | 3.2521)\n",
            "1:111 -> (2.9181 | 2.9468), (3.1508 | 3.2518)\n",
            "1:112 -> (2.8893 | 2.9466), (3.1687 | 3.2516)\n",
            "1:113 -> (2.9550 | 2.9467), (3.1324 | 3.2513)\n",
            "1:114 -> (2.8579 | 2.9464), (3.1840 | 3.2512)\n",
            "1:115 -> (2.8909 | 2.9463), (3.1997 | 3.2510)\n",
            "1:116 -> (2.9406 | 2.9463), (3.2058 | 3.2509)\n",
            "1:117 -> (2.9072 | 2.9462), (3.1218 | 3.2506)\n",
            "1:118 -> (2.9956 | 2.9463), (3.1703 | 3.2504)\n",
            "1:119 -> (2.9325 | 2.9463), (3.1816 | 3.2502)\n",
            "1:120 -> (2.9145 | 2.9462), (3.2340 | 3.2502)\n",
            "1:121 -> (2.9599 | 2.9462), (3.2286 | 3.2501)\n",
            "1:122 -> (2.9586 | 2.9463), (3.1705 | 3.2499)\n",
            "1:123 -> (2.8842 | 2.9461), (3.2320 | 3.2499)\n",
            "1:124 -> (2.8969 | 2.9460), (3.2132 | 3.2498)\n",
            "1:125 -> (2.9098 | 2.9459), (3.2414 | 3.2498)\n",
            "1:126 -> (2.9405 | 2.9459), (3.2359 | 3.2497)\n",
            "1:127 -> (2.8998 | 2.9458), (3.1596 | 3.2495)\n",
            "1:128 -> (2.9180 | 2.9457), (3.2050 | 3.2494)\n",
            "1:129 -> (2.8695 | 2.9455), (3.1904 | 3.2493)\n",
            "1:130 -> (2.9504 | 2.9455), (3.2837 | 3.2494)\n",
            "1:131 -> (2.9560 | 2.9455), (3.1607 | 3.2491)\n",
            "1:132 -> (2.8641 | 2.9453), (3.2484 | 3.2491)\n",
            "1:133 -> (3.0503 | 2.9456), (3.2199 | 3.2491)\n",
            "1:134 -> (2.9289 | 2.9456), (3.2017 | 3.2489)\n",
            "1:135 -> (2.9360 | 2.9455), (3.2252 | 3.2489)\n",
            "1:136 -> (2.9443 | 2.9455), (3.2156 | 3.2488)\n",
            "1:137 -> (2.8927 | 2.9454), (3.1307 | 3.2485)\n",
            "1:138 -> (2.9647 | 2.9454), (3.2267 | 3.2484)\n",
            "1:139 -> (2.7917 | 2.9451), (3.2178 | 3.2484)\n",
            "1:140 -> (2.9031 | 2.9450), (3.1435 | 3.2481)\n",
            "1:141 -> (2.9636 | 2.9450), (3.2524 | 3.2481)\n",
            "1:142 -> (2.9553 | 2.9450), (3.2150 | 3.2480)\n",
            "1:143 -> (2.9921 | 2.9451), (3.2273 | 3.2480)\n",
            "1:144 -> (2.9593 | 2.9452), (3.1888 | 3.2478)\n",
            "1:145 -> (2.9805 | 2.9453), (3.1924 | 3.2477)\n",
            "1:146 -> (2.8894 | 2.9451), (3.2984 | 3.2478)\n",
            "1:147 -> (2.9169 | 2.9451), (3.2752 | 3.2479)\n",
            "1:148 -> (2.9286 | 2.9450), (3.2424 | 3.2479)\n",
            "1:149 -> (2.9390 | 2.9450), (3.2333 | 3.2478)\n",
            "1:150 -> (2.8807 | 2.9448), (3.1954 | 3.2477)\n",
            "1:151 -> (2.9978 | 2.9450), (3.1437 | 3.2474)\n",
            "1:152 -> (2.9538 | 2.9450), (3.2408 | 3.2474)\n",
            "1:153 -> (2.9685 | 2.9451), (3.2031 | 3.2473)\n",
            "1:154 -> (2.9621 | 2.9451), (3.1619 | 3.2471)\n",
            "1:155 -> (2.9284 | 2.9451), (3.3083 | 3.2473)\n",
            "1:156 -> (2.8860 | 2.9449), (3.2497 | 3.2473)\n",
            "1:157 -> (2.9365 | 2.9449), (3.2268 | 3.2472)\n",
            "1:158 -> (2.9585 | 2.9449), (3.1863 | 3.2471)\n",
            "1:159 -> (2.8904 | 2.9448), (3.1645 | 3.2469)\n",
            "1:160 -> (2.9989 | 2.9449), (3.1380 | 3.2466)\n",
            "1:161 -> (2.9871 | 2.9450), (3.2056 | 3.2465)\n",
            "1:162 -> (2.9174 | 2.9450), (3.2089 | 3.2464)\n",
            "1:163 -> (2.9087 | 2.9449), (3.2456 | 3.2464)\n",
            "1:164 -> (2.9294 | 2.9448), (3.2290 | 3.2463)\n",
            "1:165 -> (2.9107 | 2.9447), (3.2212 | 3.2463)\n",
            "1:166 -> (2.9234 | 2.9447), (3.2194 | 3.2462)\n",
            "1:167 -> (2.8466 | 2.9444), (3.2312 | 3.2462)\n",
            "1:168 -> (2.9193 | 2.9444), (3.1938 | 3.2460)\n",
            "1:169 -> (2.9391 | 2.9444), (3.2823 | 3.2461)\n",
            "1:170 -> (2.9531 | 2.9444), (3.2449 | 3.2461)\n",
            "1:171 -> (2.9198 | 2.9443), (3.2706 | 3.2462)\n",
            "1:172 -> (2.8848 | 2.9442), (3.2211 | 3.2461)\n",
            "1:173 -> (2.9187 | 2.9441), (3.2417 | 3.2461)\n",
            "1:174 -> (2.8941 | 2.9440), (3.2298 | 3.2461)\n",
            "1:175 -> (2.9961 | 2.9441), (3.2158 | 3.2460)\n",
            "1:176 -> (2.9526 | 2.9441), (3.2036 | 3.2459)\n"
          ]
        }
      ],
      "source": [
        "for batch_no, (inp, tgt) in enumerate(batch_generator(\"train\"), start=1):\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(inp)\n",
        "    B, T, C = logits.shape\n",
        "    logits, tgt = logits.reshape(B * T, C), tgt.reshape(B * T)\n",
        "    train_loss = F.cross_entropy(logits, tgt)\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "    test_loss = get_test_loss()\n",
        "    train_loss, test_loss = train_loss.item(), test_loss.item()\n",
        "    mean_train_loss = (mean_train_loss or train_loss) * 0.9975 + train_loss * 0.0025\n",
        "    mean_test_loss = (mean_test_loss or test_loss) * 0.9975 + test_loss * 0.0025\n",
        "    print(\n",
        "        \"%d:%d -> (%.4f | %.4f), (%.4f | %.4f)\"\n",
        "        % (\n",
        "            batch_no // batch_to_epoch + 1,\n",
        "            batch_no % batch_to_epoch,\n",
        "            train_loss,\n",
        "            mean_train_loss,\n",
        "            test_loss,\n",
        "            mean_test_loss,\n",
        "        )\n",
        "    )\n",
        "    if last_saved_train_loss - mean_train_loss >= 0.02:\n",
        "      save_model(\"drive/MyDrive/poet_params\")\n",
        "      print(f\"saved model at train loss {mean_train_loss}\")\n",
        "      last_saved_train_loss = mean_train_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(text=\" \", max_len=200):\n",
        "  context = torch.tensor([tokenizer.encode(text)], device=device)\n",
        "  output = []\n",
        "  for _ in range(max_len):\n",
        "    logits = model(context)\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    probs = probs[:, -1:, :].view(-1, tokenizer.size)\n",
        "    next_token = torch.multinomial(probs, num_samples=1)\n",
        "    output.append(next_token.item())\n",
        "    context = torch.cat((context, next_token), dim=1)[:, -BLOCK_SIZE:]\n",
        "  return output\n",
        "print(tokenizer.decode(generate()))"
      ],
      "metadata": {
        "id": "vrPFL_Q9vuaI",
        "outputId": "e79c0f90-a7cf-46a6-e814-0b7e671136a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "veldt and broke, growing\n",
            "A little lease of deadlier words.\n",
            "Or a some unknown feeling of beauty\n",
            "That comes, indeed in many ways,\n",
            "With it has flown away.\n",
            "I have written its first song and long will never cease to run,\n",
            "We make the day for hours and time, we will never forget and we will live each other that,\n",
            "With racism doth live to another day.\n",
            "Our mind Heavenly Spirits in the air\n",
            "Of the lost and unspecial joys of God,\n",
            "Of the deepest woods of Memory! Yea, shall we see\n",
            "The harlots of the earth, porters of Wind,\n",
            "Let fall beneath the earth and he\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}